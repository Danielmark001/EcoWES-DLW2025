{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parvez\\AppData\\Local\\Temp\\ipykernel_19460\\161507741.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"machine_status\"] = df[\"machine_status\"].replace({\"NORMAL\": 0, \"RECOVERING\": 1})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets\\\\sensor.csv\")\n",
    "df = df[df[\"machine_status\"].isin([\"NORMAL\", \"RECOVERING\"])]\n",
    "df[\"machine_status\"] = df[\"machine_status\"].replace({\"NORMAL\": 0, \"RECOVERING\": 1})\n",
    "df.drop(columns=['sensor_15'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If timestamp exists, sort by it (and then drop it)\n",
    "if \"timestamp\" in df.columns:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.sort_values(\"timestamp\")\n",
    "    df = df.drop(columns=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"machine_status\"] = df[\"machine_status\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"machine_status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = [f\"sensor_0{i}\" for i in range(10)]\n",
    "sensor_columns.extend([f\"sensor_{i}\" for i in range(10, 52)])\n",
    "sensor_columns.remove(\"sensor_15\")\n",
    "label_column = \"machine_status\"   # Assumed binary label (after processing: e.g., 0 for normal, 1 for fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_00       2.549016\n",
       "sensor_01      56.727430\n",
       "sensor_02      56.032990\n",
       "sensor_03      48.220490\n",
       "sensor_04     800.000000\n",
       "sensor_05      99.999880\n",
       "sensor_06      22.251160\n",
       "sensor_07      23.596640\n",
       "sensor_08      24.348960\n",
       "sensor_09      25.000000\n",
       "sensor_10      76.106860\n",
       "sensor_11      60.000000\n",
       "sensor_12      45.000000\n",
       "sensor_13      31.187550\n",
       "sensor_14     500.000000\n",
       "sensor_16     739.741500\n",
       "sensor_17     599.999939\n",
       "sensor_18       4.873250\n",
       "sensor_19     878.917900\n",
       "sensor_20     448.907900\n",
       "sensor_21    1107.526000\n",
       "sensor_22     594.061100\n",
       "sensor_23    1227.564000\n",
       "sensor_24    1000.000000\n",
       "sensor_25     839.575000\n",
       "sensor_26    1214.420000\n",
       "sensor_27    2000.000000\n",
       "sensor_28    1841.146000\n",
       "sensor_29    1466.281000\n",
       "sensor_30    1600.000000\n",
       "sensor_31    1800.000000\n",
       "sensor_32    1839.211000\n",
       "sensor_33    1578.600000\n",
       "sensor_34     425.549800\n",
       "sensor_35     694.479126\n",
       "sensor_36     984.060700\n",
       "sensor_37     174.901200\n",
       "sensor_38     417.708300\n",
       "sensor_39     547.916600\n",
       "sensor_40     512.760400\n",
       "sensor_41     420.312500\n",
       "sensor_42     374.218800\n",
       "sensor_43     408.593700\n",
       "sensor_44    1000.000000\n",
       "sensor_45     320.312500\n",
       "sensor_46     370.370400\n",
       "sensor_47     303.530100\n",
       "sensor_48     561.632000\n",
       "sensor_49     464.409700\n",
       "sensor_50    1000.000000\n",
       "sensor_51    1000.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[sensor_columns].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Scale sensor columns\n",
    "scaler = StandardScaler()\n",
    "df[sensor_columns] = scaler.fit_transform(df[sensor_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_00     0.439173\n",
       "sensor_01     2.773572\n",
       "sensor_02     1.408796\n",
       "sensor_03     1.847194\n",
       "sensor_04     1.453500\n",
       "sensor_05     1.538055\n",
       "sensor_06     4.089005\n",
       "sensor_07     3.567216\n",
       "sensor_08     4.543642\n",
       "sensor_09     4.928316\n",
       "sensor_10     2.864207\n",
       "sensor_11     1.384962\n",
       "sensor_12     1.568508\n",
       "sensor_13     3.493251\n",
       "sensor_14     1.087796\n",
       "sensor_16     2.564314\n",
       "sensor_17     1.385069\n",
       "sensor_18     3.355255\n",
       "sensor_19     1.445221\n",
       "sensor_20     0.864002\n",
       "sensor_21     1.373361\n",
       "sensor_22     0.868969\n",
       "sensor_23     1.044995\n",
       "sensor_24     2.434374\n",
       "sensor_25     0.862269\n",
       "sensor_26     1.735272\n",
       "sensor_27     8.824201\n",
       "sensor_28     3.160588\n",
       "sensor_29     3.943207\n",
       "sensor_30     5.037543\n",
       "sensor_31     3.303555\n",
       "sensor_32     3.971877\n",
       "sensor_33     7.245212\n",
       "sensor_34     2.156527\n",
       "sensor_35     1.885840\n",
       "sensor_36     1.351261\n",
       "sensor_37     3.034647\n",
       "sensor_38    34.999445\n",
       "sensor_39    32.793238\n",
       "sensor_40    20.786444\n",
       "sensor_41    48.794452\n",
       "sensor_42    33.035598\n",
       "sensor_43    33.039799\n",
       "sensor_44    82.703563\n",
       "sensor_45    21.595592\n",
       "sensor_46    20.611461\n",
       "sensor_47    24.828369\n",
       "sensor_48     4.994549\n",
       "sensor_49    21.277252\n",
       "sensor_50    15.523135\n",
       "sensor_51     7.543451\n",
       "dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[sensor_columns].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to numpy arrays\n",
    "data_array = df[sensor_columns].values    # shape: (num_samples, num_features)\n",
    "labels_array = df[label_column].values      # shape: (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates sliding windows from the sensor data.\n",
    "    Each sample is a window of consecutive sensor readings and the label of the last time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, window_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Window of sensor readings\n",
    "        X = self.data[idx : idx + self.window_size]\n",
    "        # Label from the last time step in the window\n",
    "        y = self.labels[idx + self.window_size - 1]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, num_features, window_size):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        # CNN layers (Conv1d expects input shape: [batch, channels, sequence_length])\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding = 1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        conv1_out = window_size - 5 + 1\n",
    "        pool1_out = conv1_out // 2\n",
    "        conv2_out = pool1_out\n",
    "        pool2_out = conv2_out // 2\n",
    "        conv3_out = pool2_out - 3 + 1\n",
    "        pool3_out = conv3_out // 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        print(\"pool 1 x shape:\", x.shape)\n",
    "        x = self.pool2(self.relu(self.conv2(x)))\n",
    "        print(\"pool 2 x shape:\", x.shape)\n",
    "        x = self.pool3(self.relu(self.conv3(x)))\n",
    "        print(\"pool 3 x shape:\", x.shape)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        out = self.relu(self.fc1(h_n[-1]))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100       # Number of consecutive readings per sample\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SlidingWindowDataset(data_array, labels_array, window_size)\n",
    "# Split the dataset into training and testing sets (90% train, 10% test)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6194, 689)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the CNN+LSTM model\n",
    "num_features = len(sensor_columns)\n",
    "model = CNNLSTM(num_features=num_features, window_size=window_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features    # Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4607e-01,  5.8621e-01,  1.5406e-01,  ...,  2.3671e+00,\n",
       "          -1.6201e-15,  2.5863e-01],\n",
       "         [ 2.4607e-01,  5.8621e-01,  1.5406e-01,  ...,  2.2915e+00,\n",
       "          -1.6201e-15,  2.3399e-01],\n",
       "         [ 2.4363e-01,  5.8621e-01,  1.6590e-01,  ...,  2.2310e+00,\n",
       "          -1.6201e-15,  2.1483e-01],\n",
       "         ...,\n",
       "         [ 1.9718e-01,  8.4975e-01,  1.5406e-01,  ...,  7.9501e-01,\n",
       "          -1.6201e-15,  5.3310e-02],\n",
       "         [ 2.0940e-01,  8.4975e-01,  1.5406e-01,  ...,  7.9501e-01,\n",
       "          -1.6201e-15,  6.4261e-02],\n",
       "         [ 1.9474e-01,  8.4975e-01,  1.4223e-01,  ...,  8.1013e-01,\n",
       "          -1.6201e-15,  7.2474e-02]],\n",
       "\n",
       "        [[ 8.4741e-02,  1.3636e+00,  7.9327e-01,  ...,  3.0473e+00,\n",
       "          -1.6201e-15,  4.5574e-01],\n",
       "         [ 6.7629e-02,  1.3636e+00,  8.0510e-01,  ...,  3.0171e+00,\n",
       "          -1.6201e-15,  4.6395e-01],\n",
       "         [ 7.9851e-02,  1.3636e+00,  7.9327e-01,  ...,  3.0473e+00,\n",
       "          -1.6201e-15,  4.5300e-01],\n",
       "         ...,\n",
       "         [ 6.0297e-02,  1.1133e+00,  6.3938e-01,  ...,  8.2524e-01,\n",
       "          -1.6201e-15,  5.5977e-01],\n",
       "         [ 9.4518e-02,  1.1001e+00,  6.2755e-01,  ...,  8.2524e-01,\n",
       "          -1.6201e-15,  5.2144e-01],\n",
       "         [ 8.4741e-02,  1.1001e+00,  6.2755e-01,  ...,  1.0066e+00,\n",
       "          -1.6201e-15,  4.7490e-01]],\n",
       "\n",
       "        [[ 1.9474e-01, -6.7466e-03,  5.9203e-01,  ..., -5.6543e-01,\n",
       "          -5.3669e-01, -3.0258e-01],\n",
       "         [ 2.0696e-01, -6.7457e-03,  5.9203e-01,  ..., -5.8055e-01,\n",
       "          -5.9717e-01, -3.1901e-01],\n",
       "         [ 2.0696e-01, -6.7466e-03,  5.8020e-01,  ..., -5.8055e-01,\n",
       "          -6.3565e-01, -3.2996e-01],\n",
       "         ...,\n",
       "         [ 1.9474e-01,  7.2316e-02,  6.3938e-01,  ..., -4.2939e-01,\n",
       "          -4.7360e-02, -6.7144e-02],\n",
       "         [ 2.0940e-01,  5.9140e-02,  6.5122e-01,  ..., -4.2939e-01,\n",
       "          -1.0234e-01, -7.8096e-02],\n",
       "         [ 1.9718e-01,  7.2316e-02,  6.5122e-01,  ..., -4.1427e-01,\n",
       "          -1.4083e-01, -8.3571e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.9007e-01,  7.5751e-01,  5.0917e-01,  ...,  1.2990e-01,\n",
       "           1.0962e+00,  8.3353e-01],\n",
       "         [ 2.9007e-01,  7.5751e-01,  5.0917e-01,  ...,  1.2990e-01,\n",
       "           1.0962e+00,  8.3353e-01],\n",
       "         [ 2.9251e-01,  7.7069e-01,  5.0917e-01,  ...,  1.2990e-01,\n",
       "           1.3931e+00,  6.8023e-01],\n",
       "         ...,\n",
       "         [ 2.9985e-01,  3.6221e-01,  4.1448e-01,  ..., -2.0265e-01,\n",
       "           9.0092e-02,  1.3544e-01],\n",
       "         [ 2.8273e-01,  3.3585e-01,  4.0264e-01,  ..., -1.8753e-01,\n",
       "           1.8616e-02,  1.1354e-01],\n",
       "         [ 2.9251e-01,  3.2268e-01,  4.0264e-01,  ..., -1.2707e-01,\n",
       "          -3.6364e-02,  1.1080e-01]],\n",
       "\n",
       "        [[ 1.8985e-01,  2.3044e-01,  5.0917e-01,  ..., -3.0846e-01,\n",
       "           4.2547e-01, -4.5315e-01],\n",
       "         [ 2.1674e-01,  2.3044e-01,  4.9734e-01,  ..., -3.0846e-01,\n",
       "           4.0348e-01, -3.7102e-01],\n",
       "         [ 1.9474e-01,  2.4362e-01,  4.9734e-01,  ..., -3.0846e-01,\n",
       "           3.7599e-01, -2.7520e-01],\n",
       "         ...,\n",
       "         [ 2.0207e-01,  2.0408e-01,  4.4999e-01,  ..., -3.9916e-01,\n",
       "           5.2994e-01, -1.1095e-01],\n",
       "         [ 2.0940e-01,  1.9091e-01,  4.4999e-01,  ..., -3.9916e-01,\n",
       "           5.5743e-01, -9.7259e-02],\n",
       "         [ 1.9229e-01,  2.0408e-01,  4.4999e-01,  ..., -3.9916e-01,\n",
       "           5.9041e-01, -9.1783e-02]],\n",
       "\n",
       "        [[ 9.2073e-02,  3.2784e-02, -3.9044e-01,  ..., -6.2590e-01,\n",
       "          -2.3979e-01, -1.5130e-02],\n",
       "         [ 7.0074e-02,  4.5961e-02, -3.9044e-01,  ..., -6.2590e-01,\n",
       "          -2.2330e-01, -3.9768e-02],\n",
       "         [ 7.9851e-02,  3.2784e-02, -3.9044e-01,  ..., -6.2590e-01,\n",
       "          -1.9581e-01, -6.4407e-02],\n",
       "         ...,\n",
       "         [ 7.0074e-02,  5.9140e-02, -3.0758e-01,  ..., -6.1078e-01,\n",
       "          -5.4219e-01, -1.9034e-01],\n",
       "         [ 7.0074e-02,  5.9140e-02, -3.0758e-01,  ..., -6.1078e-01,\n",
       "          -5.4219e-01, -1.9034e-01],\n",
       "         [ 9.6963e-02,  1.2502e-01, -3.1942e-01,  ..., -5.8055e-01,\n",
       "          -5.8617e-01, -1.9034e-01]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool 1 x shape: torch.Size([32, 64, 48])\n",
      "pool 2 x shape: torch.Size([32, 128, 24])\n",
      "pool 3 x shape: torch.Size([32, 256, 11])\n",
      "outputs: tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m outputs = model(inputs)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33moutputs:\u001b[39m\u001b[33m\"\u001b[39m, outputs)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[32m     13\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\Hackathons\\Deeplearning Week 2025\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\Hackathons\\Deeplearning Week 2025\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\Hackathons\\Deeplearning Week 2025\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[39m, in \u001b[36mBCELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Computer Science\\Hackathons\\Deeplearning Week 2025\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:3569\u001b[39m, in \u001b[36mbinary_cross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3566\u001b[39m     new_size = _infer_size(target.size(), weight.size())\n\u001b[32m   3567\u001b[39m     weight = weight.expand(new_size)\n\u001b[32m-> \u001b[39m\u001b[32m3569\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # print(\"inputs:\", inputs.shape)\n",
    "        # print(\"labels:\", labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        print(\"outputs:\", outputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
